{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a06c71c6-a696-4f2d-9051-99a2bd0d215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk==3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d787648-7816-40e0-bbde-700d64865e38",
   "metadata": {},
   "source": [
    "# Setiment Analysis using Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85551e6-4143-4dee-a6b7-15971029a527",
   "metadata": {},
   "source": [
    "A large amount of data that is generated today is unstructured, which requires processing to generate insights. Some examples of unstructured data are news articles, posts on social media, and search history. The process of analyzing natural language and making sense out of it falls under the field of Natural Language Processing (NLP). Sentiment analysis is a common NLP task, which involves classifying texts or parts of texts into a pre-defined sentiment. \n",
    "\n",
    "In this project I will use the Natural Language Toolkit (NLTK), a commonly used NLP library in Python, to analyze textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc04e74-76d3-417a-811e-e4f8eb671b8f",
   "metadata": {},
   "source": [
    "I will prepare a dataset of sample tweets from the NLTK package for NLP with different data cleaning methods. Once the dataset is ready for processing, I will train a model on pre-classified tweets and use the model to classify the sample tweets into negative and positives sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c977f-4a22-45dd-8371-cfd2b958c8c2",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06501502-4302-4cb3-94d8-979634d99027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re, string\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9fb1d-2419-4b89-8b72-36f82321cdaa",
   "metadata": {},
   "source": [
    "## Tokenizing the Data\n",
    "\n",
    "Language in its original form cannot be accurately processed by a machine, so I need to process the language to make it easier for the machine to understand. The first part of making sense of the data is through a process called tokenization, or splitting strings into smaller parts called tokens.\n",
    "\n",
    "A token is a sequence of characters in text that serves as a unit. Based on how the tokens are created, they may consist of words, emoticons, hashtags, links, or even individual characters. A basic way of breaking language into tokens is by splitting the text based on whitespace and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70874116-6e3b-4581-a11c-d6a46b47c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd7c8d-20da-4e12-812e-b77a5c0de597",
   "metadata": {},
   "source": [
    "The strings() method of twitter_samples will print all of the tweets within a dataset as strings. Setting the different tweet collections as a variable will make processing and testing easier.\n",
    "\n",
    "Before using a tokenizer in NLTK, I need to download an additional resource, punkt. The punkt module is a pre-trained model that helps tokenize words and sentences. For instance, this model knows that a name may contain a period (like “S. Daityari”) and the presence of this period in a sentence does not necessarily end it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8509dde9-8d9a-4eb7-bd85-6b38dbce57f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eaaa99-6799-49cc-81a1-652967269334",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "\n",
    "Words have different forms—for instance, “ran”, “runs”, and “running” are various forms of the same verb, “run”. Depending on the requirement of your analysis, all of these versions may need to be converted to the same form, “run”. Normalization in NLP is the process of converting a word to its canonical form.\n",
    "\n",
    "There are two ways of handling normalization: stemming and lemmatization.\n",
    "\n",
    "Stemming is a process of removing affixes from a word. Stemming, working with only simple verb forms, is a heuristic process that removes the ends of words.\n",
    "\n",
    "Lemmatization normalizes word with the context of vocabulary and morphological analysis of words in text. The lemmatization algorithm analyzes the structure of the word and its context to convert it to a normalized form. Therefore, it comes at a cost of speed. A comparison of stemming and lemmatization ultimately comes down to a trade off between speed and accuracy.\n",
    "\n",
    "In this project, I'm going to use lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0392653-3a90-4076-99f4-2f200726ccac",
   "metadata": {},
   "source": [
    "wordnet is a lexical database for the English language that helps the script determine the base word. I'm using the averaged_perceptron_tagger resource to determine the context of a word in a sentence.\n",
    "\n",
    "Before running a lemmatizer, I need to determine the context for each word in your text. This is achieved by a tagging algorithm, which assesses the relative position of a word in a sentence. For this I'm going to use the pos_tag function, and provide to it a list of tokens as an argument to get the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95aed3c4-e19c-47d1-b489-fc6ca49fd311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0885f-2928-4d35-96b5-5a75642fc3cd",
   "metadata": {},
   "source": [
    "From the list of tags, here is the list of the most common items and their meaning:\n",
    "\n",
    "1. NNP: Noun, proper, singular\n",
    "2. NN: Noun, common, singular or mass\n",
    "3. IN: Preposition or conjunction, subordinating\n",
    "4. VBG: Verb, gerund or present participle\n",
    "5. VBN: Verb, past participle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b69d22-959d-40c6-91d3-72364b226605",
   "metadata": {},
   "source": [
    "In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0ad64f-4de4-4ae7-b567-5f08120685fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494665f0-ef66-4e03-a357-3a647c70dd90",
   "metadata": {},
   "source": [
    "This code imports the WordNetLemmatizer class and initializes it to a variable, lemmatizer.\n",
    "\n",
    "The function lemmatize_sentence first gets the position tag of each token of a tweet. Within the if statement, if the tag starts with NN, the token is assigned as a noun. Similarly, if the tag starts with VB, the token is assigned as a verb.You will notice that the verb being changes to its root form, be, and the noun members changes to member."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fee389-4cc0-485a-ad3b-87dbaae28237",
   "metadata": {},
   "source": [
    "I we take a look at the output we can see that the verb being changes to its root form, be, and the noun members changes to member."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe86cd-0834-46ca-a48b-a7c48ef3c5da",
   "metadata": {},
   "source": [
    "## Removing Noise from the Data\n",
    "\n",
    "In this step, I will remove noise from the dataset. Noise is any part of the text that does not add meaning or information to data.\n",
    "\n",
    "Noise is specific to each project, so what constitutes noise in one project may not be in a different project. For instance, the most common words in a language are called stop words. Some examples of stop words are “is”, “the”, and “a”. They are generally irrelevant when processing language, unless a specific use case warrants their inclusion.\n",
    "\n",
    "In this project, I will use regular expressions in Python to search for and remove these items:\n",
    "\n",
    "1. Hyperlinks - All hyperlinks in Twitter are converted to the URL shortener t.co. Therefore, keeping them in the text processing would not add any value to the analysis.\n",
    "2. Twitter handles in replies - These Twitter usernames are preceded by a @ symbol, which does not convey any meaning.\n",
    "3. Punctuation and special characters - While these often provide context to textual data, this context is often difficult to process. For simplicity, I will remove all punctuation and special characters from tweets.\n",
    "\n",
    "To remove hyperlinks, first I'm going to search for a substring that matches a URL starting with http:// or https://, followed by letters, numbers, or special characters. Once a pattern is matched, the .sub() method replaces it with an empty string.\n",
    "\n",
    "I'm also going to include normalization in this new function, so that it is simplier to use in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ebb795-45ed-4ec5-ad1c-9f8cddb7d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235b053e-2ee7-45ed-9ece-6e8ed80088c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(remove_noise(tweet_tokens[0], stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9544441-cc3f-409d-9068-7ecac79e3983",
   "metadata": {},
   "source": [
    "Now let's take a look at the comparison between initial and cleaned tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb8f59e4-7cc8-4ef2-8419-66d0adbe7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57f1c30-73ae-4fef-92ae-181f0db972a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03134e-6421-4568-9c23-3d0f3edfcaa7",
   "metadata": {},
   "source": [
    "## Determining Word Density\n",
    "\n",
    "Let's take a look at the words density of all of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89157cf3-12a7-494e-a210-0e073f3cbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cdb22eb-7c78-477d-b0ed-82f5ccf23287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "311ab052-1398-48f7-bf92-f5d3a2163894",
   "metadata": {},
   "source": [
    "Here we see that most commonly used words are emoticons with double the numbers of first proper word encounter \"thanks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52704546-ec12-4747-828f-c6df2a3a1413",
   "metadata": {},
   "source": [
    "## Preparing Data for the Model\n",
    "\n",
    "Sentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. In this project I will create a training data set to train a model and then, using supervised learning machine learning process, I will train the model to distinguish between the “positive” and “negative” sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2305b2-8745-4f30-b97a-0da80babcd77",
   "metadata": {},
   "source": [
    "## Converting Tokens to Dictionary\n",
    "\n",
    "First, I need to transform tweets from tokens to dictionary. I will be using Naive Bayes Classifier in this project and for it to work I have to give it dictionary with words as keys and True as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e74cbc0c-95a8-4050-9cc8-532f08635b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cff1dc-6534-42f0-9606-079009c1e054",
   "metadata": {},
   "source": [
    "## Splitting the Dataset for Training and Testing Model\n",
    "\n",
    "Now I will split the data into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b32006-65b5-4bbb-bebf-558130f135c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d49c0-b854-4855-bc1f-641f4b81279e",
   "metadata": {},
   "source": [
    "## Building and Testing the Model\n",
    "\n",
    "Finally, I can run the Naive Bayes Classifier on training data and then test its accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2356d0e6-a24a-41c8-a43c-b97af2fc3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9953333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2044.4 : 1.0\n",
      "                      :) = True           Positi : Negati =    988.6 : 1.0\n",
      "                     sad = True           Negati : Positi =     57.8 : 1.0\n",
      "                followed = True           Negati : Positi =     26.1 : 1.0\n",
      "                    sick = True           Negati : Positi =     20.1 : 1.0\n",
      "                  arrive = True           Positi : Negati =     19.2 : 1.0\n",
      "               community = True           Positi : Negati =     15.8 : 1.0\n",
      "                 welcome = True           Positi : Negati =     15.7 : 1.0\n",
      "                     x15 = True           Negati : Positi =     13.5 : 1.0\n",
      "                    glad = True           Positi : Negati =     13.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d95281e-1d7e-4bf0-9b3a-38c685fe93e8",
   "metadata": {},
   "source": [
    "Accuracy is a percentage of tweets in the testing dataset for which the model correctly predicted the sentiment. 99.5% accuracy on the test set is very good.\n",
    "\n",
    "Let's check how our model does on randomly input tweets outside of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a445f06f-56f7-4800-bc5f-0f24ee6124ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43daee19-8606-4cf1-b3c6-1038850f4794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e143c-98ac-4b9a-aec0-e16685b95af4",
   "metadata": {},
   "source": [
    "And model does pretty well with tweets input by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ceb26f-740b-4e92-abbd-443bf61c7fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
